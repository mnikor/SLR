!pip install openai
!pip install python-docx
!pip install biopython
import openai
from docx import Document
from Bio import Entrez

# Initialize Document
doc = Document()
doc.add_heading('Research Analysis', 0)

# Configuration
openai.api_key = ""
Entrez.email = "your_email@example.com"
Entrez.api_key = "e9e5a6445de37219ca2be227c1b164cbf808"
main_research_question = "What is the impact of bleeding and the fear of bleeding in AF patients using DOACs?"

# Generate Alternative Research Questions using GPT-3.5 Turbo
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo-16k",
    messages=[
        {"role": "system", "content": "You are a helpful research assistant."},
        {"role": "user", "content": f"Generate alternative research questions based on the main question: {main_research_question}"}
    ]
)
alternative_questions = response['choices'][0]['message']['content'].strip().split("\n")
doc.add_heading('Main Research Question:', level=1)
doc.add_paragraph(main_research_question)
doc.add_heading('Alternative Research Questions:', level=1)
for question in alternative_questions:
    doc.add_paragraph(question)

# Generate Keywords using GPT-3.5 Turbo
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo-16k",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": f"Generate keywords related to the research questions: {main_research_question}, {', '.join(alternative_questions)}"}
    ]
)
keywords = response['choices'][0]['message']['content'].strip().split(", ")
doc.add_heading('Keywords:', level=1)
doc.add_paragraph(", ".join(keywords))

# ... [Rest of the code remains the same, including the scoring functions and main process] ...


# Scoring functions using GPT model
def score_relevance(abstract_text, question):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-16k",
        prompt=f"Rate the relevance of the following abstract to the research question '{question}':\n\n{abstract_text}\n\nRate as: Very Relevant, Somewhat Relevant, Not Relevant.",
        max_tokens=50
    )
    relevance_response = response.choices[0].text.strip().lower()
    if "very relevant" in relevance_response:
        return 3
    elif "somewhat relevant" in relevance_response:
        return 2
    else:
        return 1

def score_relevance(abstract_text, question):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-16k",
        messages=[
            {"role": "system", "content": "You are a research evaluator."},
            {"role": "user", "content": f"Rate the relevance of the following abstract to the research question '{question}':\n\n{abstract_text}\n\nRate as: Very Relevant, Somewhat Relevant, Not Relevant."}
        ]
    )
    relevance_response = response['choices'][0]['message']['content'].strip().lower()
    if "very relevant" in relevance_response:
        return 3
    elif "somewhat relevant" in relevance_response:
        return 2
    else:
        return 1

def score_study_design(abstract_text):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-16k",
        messages=[
            {"role": "system", "content": "You are a research evaluator."},
            {"role": "user", "content": f"Identify the study design from the following abstract:\n\n{abstract_text}\n\nOptions: Randomized Controlled Trial, Cohort Study, Systematic Review, Others."}
        ]
    )
    design_response = response['choices'][0]['message']['content'].strip().lower()
    if "randomized controlled trial" in design_response or "systematic review" in design_response:
        return 3
    elif "cohort study" in design_response:
        return 2
    else:
        return 1

def score_quality(abstract_text):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-16k",
        messages=[
            {"role": "system", "content": "You are a research evaluator."},
            {"role": "user", "content": f"Rate the quality of the following abstract based on its content:\n\n{abstract_text}\n\nRate as: High-Quality, Moderate Quality, Low Quality."}
        ]
    )
    quality_response = response['choices'][0]['message']['content'].strip().lower()
    if "high-quality" in quality_response:
        return 3
    elif "moderate quality" in quality_response:
        return 2
    else:
        return 1

def score_population(abstract_text, desired_population):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-16k",
        messages=[
            {"role": "system", "content": "You are a research evaluator."},
            {"role": "user", "content": f"Does the following abstract focus on {desired_population}?\n\n{abstract_text}\n\nAnswer as: Directly relevant, Somewhat relevant, Not relevant."}
        ]
    )
    population_response = response['choices'][0]['message']['content'].strip().lower()
    if "directly relevant" in population_response:
        return 3
    elif "somewhat relevant" in population_response:
        return 2
    else:
        return 1


def fetch_and_score_paper_details(paper_id, doc, main_research_question, desired_population):
    try:
        handle = Entrez.efetch(db="pubmed", id=paper_id, rettype="medline", retmode="xml")
        records = Entrez.read(handle)
        
        record = records['PubmedArticle'][0]
        article_id = record['MedlineCitation']['PMID']
        title = record['MedlineCitation']['Article']['ArticleTitle']
        
        # Extracting and combining all sections of the abstract with section names
        if 'Abstract' in record['MedlineCitation']['Article']:
            abstract_sections = record['MedlineCitation']['Article']['Abstract']['AbstractText']
            
            # Check if each section has a label (like 'Background', 'Methods', etc.) and include it
            abstract = " ".join([f"{section.attributes['Label'] if 'Label' in section.attributes else 'Abstract'}: {section}" for section in abstract_sections])
        else:
            abstract = "N/A"
        
        authors_list = record['MedlineCitation']['Article']['AuthorList']
        authors = ', '.join([f"{author.get('ForeName', 'N/A')} {author.get('LastName', 'N/A')}" for author in authors_list])
        
        if 'ArticleDate' in record['MedlineCitation']['Article']:
            date = record['MedlineCitation']['Article']['ArticleDate'][0]['Year'] if record['MedlineCitation']['Article']['ArticleDate'] else "N/A"
        else:
            date = "N/A"

        # Scoring Logic
        scores = {
            'relevance': score_relevance(abstract, main_research_question),
            'design': score_study_design(abstract),
            'quality': score_quality(abstract),
            'recency': score_recency(date),
            'population': score_population(abstract, desired_population)
        }
        
        total_score = sum(scores.values())
        
        abstract_info = {
            'article_id': article_id,
            'title': title,
            'abstract': abstract,
            'authors': authors,
            'date': date,
            'scores': scores,
            'total_score': total_score
        }

        return abstract_info

    except Exception as e:
        print(f"Error processing paper with ID {paper_id}: {e}")
        return {
            'article_id': paper_id,
            'title': "Error",
            'abstract': "Error",
            'authors': "Error",
            'date': "Error",
            'scores': {
                'relevance': 0,
                'design': 0,
                'quality': 0,
                'recency': 0,
                'population': 0
            },
            'total_score': 0
        }


# Document abstracts and scores in Word
def document_abstracts(doc, abstracts):
    table = doc.add_table(rows=1, cols=7)
    table.style = 'Table Grid'
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = 'Abstract Title'
    hdr_cells[1].text = 'Relevance'
    hdr_cells[2].text = 'Design'
    hdr_cells[3].text = 'Quality'
    hdr_cells[4].text = 'Recency'
    hdr_cells[5].text = 'Population'
    hdr_cells[6].text = 'Total Score'
    sorted_abstracts = sorted(abstracts, key=lambda x: x['total_score'], reverse=True)
    for abstract_info in sorted_abstracts:
        row_cells = table.add_row().cells
        row_cells[0].text = abstract_info['title']
        row_cells[1].text = str(abstract_info['scores']['relevance'])
        row_cells[2].text = str(abstract_info['scores']['design'])
        row_cells[3].text = str(abstract_info['scores']['quality'])
        row_cells[4].text = str(abstract_info['scores']['recency'])
        row_cells[5].text = str(abstract_info['scores']['population'])
        row_cells[6].text = str(abstract_info['total_score'])
    doc.add_paragraph("\n")
    for abstract_info in sorted_abstracts[:10]:
        doc.add_heading(f"Paper ID: {abstract_info['article_id']}", level=1)
        doc.add_paragraph(f"Title: {abstract_info['title']}")
        doc.add_paragraph(f"Abstract: {abstract_info['abstract']}")
        doc.add_paragraph(f"Authors: {abstract_info['authors']}")
        doc.add_paragraph(f"Date: {abstract_info['date']}")
        doc.add_paragraph("\n")

# Main Process
desired_population = "AF patients"
abstracts_info = []
search_query = "(impact of bleeding AND AF patients AND DOACs) OR (fear of bleeding AND treatment outcomes)"
handle = Entrez.esearch(db="pubmed", term=search_query, retmax=50)
record = Entrez.read(handle)
paper_ids = record["IdList"]
for paper_id in paper_ids:
    abstract_info = fetch_and_score_paper_details(paper_id, doc, main_research_question, desired_population)
    abstracts_info.append(abstract_info)
document_abstracts(doc, abstracts_info)
doc.save("/content/drive/MyDrive/SLR_Report_with_Scores.docx")
